## Part 1 : Import Libraries and Configure Environment
import os
import sklearn
import numpy as np # ì„ í˜•ëŒ€ìˆ˜ ë° ìˆ˜ì¹˜ ê³„ì‚°ìš©
import pandas as pd # ë°ì´í„° ì²˜ë¦¬, CSV íŒŒì¼ ì…ì¶œë ¥ ë“±
import seaborn as sns # í†µê³„ì  ë°ì´í„° ì‹œê°í™”
import matplotlib.pyplot as plt # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬


# í˜„ì¬ .py íŒŒì¼ì´ ìœ„ì¹˜í•œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
base_path = os.path.dirname(os.path.abspath(__file__))

# íŒŒì¼ ê²½ë¡œ êµ¬ì„±
train_path = os.path.join(base_path, 'playground-series-s4e9', 'train.csv')
test_path = os.path.join(base_path, 'playground-series-s4e9', 'test.csv')
sample_submission_path = os.path.join(base_path, 'playground-series-s4e9', 'sample_submission.csv')

train = pd.read_csv(train_path)
test = pd.read_csv(test_path)

# íŒë‹¤ìŠ¤ ì¶œë ¥ ì˜µì…˜ ì„¤ì • :  í–‰,ì—´ ê° ìµœëŒ€ 100ê°œ ì¶œë ¥
pd.options.display.max_columns = 100
pd.options.display.max_rows = 100

import warnings
warnings.simplefilter(action='ignore',category=FutureWarning)
pd.options.mode.chained_assignment = None

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
train = pd.read_csv(train_path, index_col='id')
test = pd.read_csv(test_path, index_col='id')

# ì „ì²˜ë¦¬ë¥¼ ìœ„í•´ train ê³¼ test í•©ì¹˜ê¸°
df = pd.concat([train, test])

# ìƒìœ„ 5ê°œ í–‰ ì¶œë ¥ (0~4ë²ˆ ì¸ë±ìŠ¤ )
df.head()

## Part 3: ë³€ìˆ˜ ì‹ë³„ ë° ë²”ì£¼í™”

# ì „ì²´ ì»¬ëŸ¼ ì¤‘ 'price' ë§Œ ì œì™¸í•œ ë³€ìˆ˜ ì¶”ì¶œ
features = df.columns.to_list() # ì „ì²´ ì»¬ëŸ¼ëª…ì„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
target = 'price' # íƒ€ê²Ÿ ë³€ìˆ˜ ì§€ì • 
features.remove(target) # ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ì— 'price' ì œê±°

# ë³€ìˆ˜ë¥¼ íƒ€ì… ê¸°ì¤€ìœ¼ ë‚˜ëˆ„ê¸°
catFeatures = df[features].select_dtypes(include='object').columns.to_list()
numFeatures = df[features].select_dtypes(include='int64').columns.to_list()

print("Categorical Features:", catFeatures)
print("Numerical Features:", numFeatures)

## Part 4: ë°ì´í„° ì •ë¦¬ ë°  ì „ì²˜ë¦¬

# ê²°ì¸¡ê°’ ë°  ì˜ëª»ëœ 'fuel_type' ë°ì´í„° ì²˜ë¦¬
df['fuel_type'].fillna('Electric',inplace=True) # ê²°ì¸¡ê°’ì¼ ê²½ìš° 'Electric'ì„ ë„£ì–´ì£¼ê¸°
df['fuel_type'].replace('-', 'Unknown',inplace=True) # ì—†ëŠ” ê°’ì˜ ê²½ìš° 'Unknown'ì„ ë„£ì–´ì£¼ê¸°

# 'engine' ë³€ìˆ˜ì— ìˆ«ì ì •ë³´ ì¶”ì¶œ  
df['horsepower'] = df['engine'].str.extract(r'(\d+).0HP').astype('Int64') # '300.0HP' â†’ 300
df['engine_size'] = df['engine'].str.extract(r'(\d+\.?\d+)\s*(?:L|Liters)').astype(float) # '3.5L' ë˜ëŠ” '3.5 Liters' â†’ 3.5
df['cylinders'] = df['engine'].str.extract(r'(?:V(\d+)|W(\d+)|I(\d+)|H(\d+)|(\d+)\s*Cylinder)').bfill(axis=1).iloc[:, 0].astype('Int64') # V6, 4 Cylinder ë“± â†’ 6, 4 ë“±
df['gears'] = df['transmission'].str.extract(r'(\d+)-Speed').astype('Int64') # '8-Speed Automatic' â†’ 8

# ë³€ì†ê¸° ì¢…ë¥˜ ë¶„ë¥˜
conditions = [
    df['transmission'].str.contains('AT|Automatic|AT', case = False, na=False),
    df['transmission'].str.contains('Manual|M/T|Mt', case=False, na=False)
]


# ìƒˆ ì»¬ëŸ¼ ìƒì„± ('Automatic', 'Manual', 'Other')
choices = ['Automatic','Manual']
df['transmission_type'] = np.select(conditions, choices, default ='Other')
# 'clean_title'ê³¼ 'accident' ê°’ì„ ì¡°í•©í•œ ìƒˆë¡œìš´ ë³€ìˆ˜ ìƒì„±
df['clean_title*no_accident'] = np.where(((df['clean_title'] == 'Yes') & (df['accident'] == 'None reported')) == True, 'Yes','No')

df.head()


## Part 5 : Handling High Cardinality Features

high_cardinality_features = ['model','engine','ext_col','int_col']
threshold = 100 # ê¸°ì¤€ ë¹ˆë„ ìˆ˜ ì„¤ì •

for feature in high_cardinality_features:
    counts = df[features].value_counts() # ê° ë²ˆì£¼ì˜ ë“±ì¥ íšŸìˆ˜
    scarse_categories = counts[counts < threshold].index # ê¸°ì¤€ ë¹ˆë„ ìˆ˜ ë³´ë‹¤ ì ì€ ë²”ì£¼ ì¶”ì¶œ
    df[feature] = df[feature].apply(lambda x: 'Other' if x in scarse_categories else x) # í¬ê·€ê°’ì„ 'Other'ë¡œ ì¹˜í™˜

df[high_cardinality_features].nunique() # ì»¬ëŸ¼ ì¤‘ ì¹´ë””ë„ë¦¬í‹°ê°€ 100ê°œ ì´ìƒì¸ ì»¬ëŸ¼ë“¤ë§Œ ì¶”ì¶œ
    
## Part 6: Encoding Categorical Variables

from sklearn.preprocessing import LabelEncoder # ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ìˆ«ì ë³€í™˜

carFeatures = df.select_dtypes(include="object").columns.to_list() # objectíƒ€ì… ì»¬ëŸ¼ë“¤ ë¦¬ìŠ¤íŠ¸ ì €ì¥ 
le = LabelEncoder() 
#   ì˜ˆ: â€œDieselâ€, â€œElectricâ€, â€œGasolineâ€ â†’ 0, 1, 2 ë¡œ ë³€í™˜
for feature in catFeatures:
    df[feature] = le.fit_transform(df[feature])

df['cylinders'].fillna(0, inplace=True)
df['gears'].fillna(0, inplace=True)

df.head()

## Part 7: ë°ì´í„° ê²°ì¸¡ê°’ ì˜ˆì¸¡

from sklearn.impute import SimpleImputer


train_df = df.iloc[train.index]
test_df = df.iloc[test.index]

imputer = SimpleImputer(strategy='median')
train_df[['horsepower', 'engine_size']] = imputer.fit_transform(train_df[['horsepower', 'engine_size']])
test_df[['horsepower', 'engine_size']] = imputer.transform(test_df[['horsepower', 'engine_size']])

train_df.head()

## Part 8: ë°ì´í„° ì‹œê°í™”

# íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ë¶„í¬ ì‹œê°í™”
plt.figure(figsize=(8,4)) # ê·¸ë˜í”„ í¬ê¸°ì„¤ì • 
sns.histplot(train_df['price'], kde=True, color='blue') # ê°€ê²© ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ +  ë°€ë„ ê³¡ì„ 
plt.title('Distribution of Car Prices')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

# ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
numeric_df = train_df.select_dtypes(include=['number'])

# ë³€ìˆ˜ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ì‹œê°í™”
plt.figure(figsize=(12, 8))
corr_matrix = numeric_df.corr() 
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

## Part 9: ëª¨ë¸ í•™ìŠµ

from sklearn.ensemble import RandomForestRegressor  
from catboost import CatBoostRegressor  

mod1 = RandomForestRegressor(n_estimators=100, random_state=42)  
# ğŸŒ² 100ê°œì˜ ê²°ì • íŠ¸ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ëœë¤í¬ë ˆìŠ¤íŠ¸ íšŒê·€ ëª¨ë¸ ìƒì„±
# random_state=42ëŠ” ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ ê³ ì •

mod2 = CatBoostRegressor(
    iterations=500, # 500ë²ˆ ë°˜ë³µ
    learning_rate=0.1, # í•™ìŠµë¥  ì„¤ì • (í•œ ë²ˆì— ì–¼ë§ˆë‚˜ ì—…ë°ì´íŠ¸í• ì§€)
    depth=6, # íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´
    random_seed=42, # # ê²°ê³¼ ì¼ê´€ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ ê³ ì •
    verbose=0
)


categorical_columns = train_df.select_dtypes(include=['object','category']).columns.tolist() # ë²”ì£¼í˜• ë³€ìˆ˜ ì¶”ì¶œ -> brand', 'fuel_type', 'transmission_type'
catFeatures = [feature for feature in categorical_columns if feature in features] # features ë¦¬ìŠ¤íŠ¸ì™€ ê²¹ì¹˜ëŠ” ë²”ì£¼í˜• ë³€ìˆ˜ ì¶”ì¶œ ->

mod1.fit(train_df[features], train_df[target])
mod2.fit(train_df[features], train_df[target], cat_features=catFeatures)

import lightgbm as lgb
import catboost as ctb

mod1 = lgb.LGBMRegressor(n_estimators=430,
                        reg_alpha=72,
                        reg_lambda=57,
                        colsample_bytree = 0.43,
                        aubsample=0.96,
                        learning_rate = 0.025,
                        max_depth=8,
                        num_leaves=583,
                        min_child_samples=320,
                        verbose=-1)

mod2 = ctb.CatBoostRegressor(iterations=475,
                             subsample=0.55,
                             colsample_bylevel=0.37,
                             min_data_in_leaf=210,
                             learning_rate=0.03,
                             l2_leaf_reg=56,
                             depth=10,
                             verbose=False)

mod1.fit(train_df[features], train_df[target])
mod2.fit(train_df[features], train_df[target], cat_features=catFeatures)

## Part 10: í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™”

lgb.plot_importance(mod1, importance_type='split', figsize=(10, 6))
plt.title('LightGBM Feature Importance')
plt.show()

## Part 11: ì˜ˆì¸¡ ë° ì œì¶œ

submission_lgb = pd.read_csv(sample_submission_path)

submission_lgb['price'] = 0.8 * mod1.predict(test_df[features])
+0.2*mod2.predict(test_df[features])

submission_lgb.to_csv('submission.csv')

