

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
import glob
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

base_path = '/kaggle/input/home-credit-credit-risk-model-stability/csv_files'
train_path = f'{base_path}/train'
test_path = f'{base_path}/test'

print("=== ìƒˆë¡œìš´ Home Credit ë°ì´í„°ì…‹ ë¶„ì„ ===")

# 1. ê¸°ë³¸ í…Œì´ë¸” ë¡œë“œ (Base tables)
def load_base_tables():
    try:
        train_base = pd.read_csv(f'{train_path}/train_base.csv')
        test_base = pd.read_csv(f'{test_path}/test_base.csv')
        
        print(f"Train base: {train_base.shape}")
        print(f"Test base: {test_base.shape}")
        
        return train_base, test_base
    except Exception as e:
        print(f"Base table ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None, None

train_base, test_base = load_base_tables()

if train_base is not None:
    print(f"\n=== Base Table ì •ë³´ ===")
    print(f"Train cases: {len(train_base):,}")
    print(f"Test cases: {len(test_base):,}")
    print(f"Train columns: {list(train_base.columns)}")
    print(f"Test columns: {list(test_base.columns)}")
    
    # íƒ€ê²Ÿ ë¶„í¬ í™•ì¸
    if 'target' in train_base.columns:
        target_dist = train_base['target'].value_counts()
        print(f"\nTarget distribution: \n{target_dist}")
        print(f"Default rate: {train_base['target'].mean():.4f}")

# 2. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  í…Œì´ë¸” íƒìƒ‰
def explore_available_tables():
    print("\n=== ì‚¬ìš© ê°€ëŠ¥í•œ í…Œì´ë¸” íƒìƒ‰ ===")
    
    train_files = glob.glob(f'{train_path}/*.csv')
    test_files = glob.glob(f'{test_path}/*.csv')
    
    print(f"Train íŒŒì¼ ìˆ˜: {len(train_files)}")
    print(f"Test íŒŒì¼ ìˆ˜: {len(test_files)}")
    
    # íŒŒì¼ëª… íŒ¨í„´ ë¶„ì„
    train_file_names = [os.path.basename(f) for f in train_files]
    test_file_names = [os.path.basename(f) for f in test_files]
    
    print(f"\nTrain íŒŒì¼ë“¤ (ì²˜ìŒ 10ê°œ):")
    for i, fname in enumerate(train_file_names[:10]):
        print(f"  {i+1}. {fname}")
    
    print(f"\nTest íŒŒì¼ë“¤ (ì²˜ìŒ 10ê°œ):")
    for i, fname in enumerate(test_file_names[:10]):
        print(f"  {i+1}. {fname}")
    
    return train_file_names, test_file_names

train_files, test_files = explore_available_tables()

# 3. ì£¼ìš” í…Œì´ë¸” ê·¸ë£¹ë³„ ë¡œë“œ í•¨ìˆ˜
def load_table_group(group_name, max_files=3):
    train_group_files = [f for f in train_files if group_name in f]
    test_group_files = [f for f in test_files if group_name in f]
    
    print(f"\n=== {group_name} ê·¸ë£¹ ë¶„ì„ ===")
    print(f"Train íŒŒì¼: {len(train_group_files)}ê°œ")
    print(f"Test íŒŒì¼: {len(test_group_files)}ê°œ")
    
    train_dfs = []
    test_dfs = []
    
    # ìµœëŒ€ max_filesê°œê¹Œì§€ë§Œ ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)
    for i, fname in enumerate(train_group_files[:max_files]):
        try:
            df = pd.read_csv(f'{train_path}/{fname}')
            train_dfs.append(df)
            print(f"  âœ“ {fname}: {df.shape}")
        except Exception as e:
            print(f"  âœ— {fname}: ë¡œë“œ ì‹¤íŒ¨ - {e}")
    
    for i, fname in enumerate(test_group_files[:max_files]):
        try:
            df = pd.read_csv(f'{test_path}/{fname}')
            test_dfs.append(df)
            print(f"  âœ“ {fname}: {df.shape}")
        except Exception as e:
            print(f"  âœ— {fname}: ë¡œë“œ ì‹¤íŒ¨ - {e}")
    
    return train_dfs, test_dfs

# 4. ì£¼ìš” í…Œì´ë¸” ê·¸ë£¹ë“¤ ë¶„ì„
table_groups = ['static_0', 'static_cb_0', 'applprev_1', 'person_1', 'credit_bureau_a_1']

loaded_tables = {}
for group in table_groups:
    train_dfs, test_dfs = load_table_group(group, max_files=2)
    loaded_tables[group] = {'train': train_dfs, 'test': test_dfs}

# 5. ê¸°ë³¸ ì•ˆì •ì„± ë¶„ì„ - Base Table ì¤‘ì‹¬
def analyze_base_stability(train_base, test_base):
    if train_base is None or test_base is None:
        print("Base tableì´ ì—†ì–´ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    print("\n=== Base Table ì•ˆì •ì„± ë¶„ì„ ===")
    
    # ê³µí†µ ì»¬ëŸ¼ í™•ì¸
    common_cols = set(train_base.columns) & set(test_base.columns)
    train_only = set(train_base.columns) - set(test_base.columns)
    test_only = set(test_base.columns) - set(train_base.columns)
    
    print(f"ê³µí†µ ì»¬ëŸ¼: {len(common_cols)}ê°œ")
    print(f"Trainì—ë§Œ ìˆëŠ” ì»¬ëŸ¼: {len(train_only)}ê°œ - {list(train_only)}")
    print(f"Testì—ë§Œ ìˆëŠ” ì»¬ëŸ¼: {len(test_only)}ê°œ - {list(test_only)}")
    
    # WEEK_NUM ë¶„í¬ ë¶„ì„ (ì‹œê°„ ì•ˆì •ì„±ì˜ í•µì‹¬)
    if 'WEEK_NUM' in common_cols:
        print(f"\n=== WEEK_NUM ë¶„í¬ ë¶„ì„ ===")
        train_weeks = train_base['WEEK_NUM'].describe()
        test_weeks = test_base['WEEK_NUM'].describe()
        
        print(f"Train WEEK_NUM: {train_weeks['min']:.0f} ~ {train_weeks['max']:.0f}")
        print(f"Test WEEK_NUM: {test_weeks['min']:.0f} ~ {test_weeks['max']:.0f}")
        
        # WEEK_NUMë³„ íƒ€ê²Ÿ ë¶„í¬ (ì‹œê°„ì— ë”°ë¥¸ íƒ€ê²Ÿ ì•ˆì •ì„±)
        if 'target' in train_base.columns:
            weekly_target = train_base.groupby('WEEK_NUM')['target'].agg(['count', 'mean']).reset_index()
            
            fig, axes = plt.subplots(2, 1, figsize=(15, 10))
            
            # ì£¼ë³„ ì¼€ì´ìŠ¤ ìˆ˜
            axes[0].plot(weekly_target['WEEK_NUM'], weekly_target['count'], marker='o', alpha=0.7)
            axes[0].set_title('Cases per Week')
            axes[0].set_xlabel('Week Number')
            axes[0].set_ylabel('Number of Cases')
            axes[0].grid(True, alpha=0.3)
            
            # ì£¼ë³„ íƒ€ê²Ÿ ë¹„ìœ¨
            axes[1].plot(weekly_target['WEEK_NUM'], weekly_target['mean'], marker='o', alpha=0.7, color='red')
            axes[1].set_title('Default Rate by Week')
            axes[1].set_xlabel('Week Number')
            axes[1].set_ylabel('Default Rate')
            axes[1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.show()
    
    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ì˜ ê¸°ë³¸ ë¶„í¬ ë¹„êµ
    numeric_cols = [col for col in common_cols if train_base[col].dtype in ['int64', 'float64']]
    numeric_cols = [col for col in numeric_cols if col not in ['case_id', 'WEEK_NUM']]
    
    if len(numeric_cols) > 0:
        print(f"\nìˆ˜ì¹˜í˜• ë³€ìˆ˜ {len(numeric_cols)}ê°œ ë°œê²¬")
        
        # ì²˜ìŒ 6ê°œ ë³€ìˆ˜ë§Œ ë¶„í¬ ë¹„êµ
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        axes = axes.ravel()
        
        for i, col in enumerate(numeric_cols[:6]):
            if i < 6:
                train_vals = train_base[col].dropna()
                test_vals = test_base[col].dropna()
                
                if len(train_vals) > 0 and len(test_vals) > 0:
                    axes[i].hist(train_vals, bins=50, alpha=0.6, label='Train', density=True)
                    axes[i].hist(test_vals, bins=50, alpha=0.6, label='Test', density=True)
                    axes[i].set_title(f'{col} Distribution')
                    axes[i].legend()
                    axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

analyze_base_stability(train_base, test_base)

# 6. Population Stability Index (PSI) ê³„ì‚°
def calculate_psi(expected, actual, bins=10):
    def psi_score(expected_array, actual_array, bins):
        # ìœ íš¨í•œ ê°’ë§Œ ì‚¬ìš©
        expected_clean = expected_array[~np.isnan(expected_array)]
        actual_clean = actual_array[~np.isnan(actual_array)]
        
        if len(expected_clean) == 0 or len(actual_clean) == 0:
            return np.nan, None, None
        
        # êµ¬ê°„ ì„¤ì •
        breakpoints = np.arange(0, bins + 1) / bins * 100
        breakpoints = np.percentile(expected_clean, breakpoints)
        breakpoints[0] = -np.inf
        breakpoints[-1] = np.inf
        
        # ê° êµ¬ê°„ë³„ ë¹„ìœ¨ ê³„ì‚°
        expected_percents = pd.cut(expected_clean, breakpoints).value_counts().sort_index() / len(expected_clean)
        actual_percents = pd.cut(actual_clean, breakpoints).value_counts().sort_index() / len(actual_clean)
        
        # 0ì¸ ê°’ ì²˜ë¦¬ (ì‘ì€ ê°’ìœ¼ë¡œ ëŒ€ì²´)
        expected_percents = expected_percents.replace(0, 1e-6)
        actual_percents = actual_percents.replace(0, 1e-6)
        
        # PSI ê³„ì‚°
        psi = sum((actual_percents - expected_percents) * np.log(actual_percents / expected_percents))
        return psi, expected_percents, actual_percents
    
    return psi_score(expected, actual, bins)

# 7. ë‹¤ì¤‘ í…Œì´ë¸” ì•ˆì •ì„± ë¶„ì„
def analyze_multi_table_stability(loaded_tables):
    print("\n=== ë‹¤ì¤‘ í…Œì´ë¸” ì•ˆì •ì„± ë¶„ì„ ===")
    
    stability_summary = []
    
    for group_name, tables in loaded_tables.items():
        print(f"\n--- {group_name} ê·¸ë£¹ ---")
        
        train_dfs = tables['train']
        test_dfs = tables['test']
        
        if len(train_dfs) > 0 and len(test_dfs) > 0:
            # ì²« ë²ˆì§¸ í…Œì´ë¸”ë¡œ ë¶„ì„
            train_df = train_dfs[0]
            test_df = test_dfs[0]
            
            print(f"ë¶„ì„ í…Œì´ë¸”: Train {train_df.shape}, Test {test_df.shape}")
            
            # ê³µí†µ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì°¾ê¸°
            common_cols = set(train_df.columns) & set(test_df.columns)
            numeric_cols = [col for col in common_cols 
                          if train_df[col].dtype in ['int64', 'float64'] 
                          and col not in ['case_id', 'WEEK_NUM', 'num_group1', 'num_group2']]
            
            if len(numeric_cols) > 0:
                print(f"ë¶„ì„í•  ìˆ˜ì¹˜í˜• ë³€ìˆ˜: {len(numeric_cols)}ê°œ")
                
                # PSI ê³„ì‚°
                for col in numeric_cols[:5]:  # ì²˜ìŒ 5ê°œë§Œ
                    try:
                        train_vals = train_df[col].values
                        test_vals = test_df[col].values
                        
                        psi, _, _ = calculate_psi(train_vals, test_vals)
                        
                        if not np.isnan(psi):
                            stability = 'Stable' if psi < 0.1 else 'Moderately Unstable' if psi < 0.25 else 'Highly Unstable'
                            stability_summary.append({
                                'table_group': group_name,
                                'feature': col,
                                'psi': psi,
                                'stability': stability
                            })
                            print(f"  {col}: PSI = {psi:.4f} ({stability})")
                    except Exception as e:
                        print(f"  {col}: ê³„ì‚° ì˜¤ë¥˜ - {e}")
    
    # ì•ˆì •ì„± ìš”ì•½
    if len(stability_summary) > 0:
        stability_df = pd.DataFrame(stability_summary)
        
        print(f"\n=== ì „ì²´ ì•ˆì •ì„± ìš”ì•½ ===")
        stability_counts = stability_df['stability'].value_counts()
        print(stability_counts)
        
        # ìƒìœ„ ë¶ˆì•ˆì • í”¼ì²˜ë“¤
        top_unstable = stability_df.nlargest(10, 'psi')
        print(f"\nìƒìœ„ 10ê°œ ë¶ˆì•ˆì • í”¼ì²˜:")
        print(top_unstable[['table_group', 'feature', 'psi', 'stability']])
        
        # ì‹œê°í™”
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        # PSI ë¶„í¬
        stability_df['psi'].hist(bins=20, ax=axes[0])
        axes[0].set_title('PSI Distribution')
        axes[0].set_xlabel('PSI Value')
        axes[0].set_ylabel('Count')
        axes[0].axvline(x=0.1, color='orange', linestyle='--', label='Moderate Threshold')
        axes[0].axvline(x=0.25, color='red', linestyle='--', label='High Threshold')
        axes[0].legend()
        
        # ì•ˆì •ì„± ë¶„í¬
        stability_counts.plot(kind='pie', ax=axes[1], autopct='%1.1f%%')
        axes[1].set_title('Feature Stability Distribution')
        
        plt.tight_layout()
        plt.show()
        
        return stability_df
    
    return pd.DataFrame()

stability_results = analyze_multi_table_stability(loaded_tables)

# 8. ì‹œê°„ë³„ ë°ì´í„° ë³¼ë¥¨ ì•ˆì •ì„± ë¶„ì„
def analyze_temporal_volume_stability():
    if train_base is None:
        return
    
    print("\n=== ì‹œê°„ë³„ ë°ì´í„° ë³¼ë¥¨ ì•ˆì •ì„± ===")
    
    if 'WEEK_NUM' in train_base.columns:
        # ì£¼ë³„ ë°ì´í„° ë³¼ë¥¨
        weekly_volume = train_base.groupby('WEEK_NUM').size()
        
        print(f"ì „ì²´ ì£¼ ìˆ˜: {len(weekly_volume)}")
        print(f"í‰ê·  ì£¼ë³„ ì¼€ì´ìŠ¤: {weekly_volume.mean():.1f}")
        print(f"ì£¼ë³„ ì¼€ì´ìŠ¤ í‘œì¤€í¸ì°¨: {weekly_volume.std():.1f}")
        print(f"ë³€ë™ê³„ìˆ˜ (CV): {weekly_volume.std()/weekly_volume.mean():.3f}")
        
        # ì‹œê°í™”
        fig, axes = plt.subplots(2, 1, figsize=(15, 10))
        
        # ì£¼ë³„ ë³¼ë¥¨ ì¶”ì´
        weekly_volume.plot(ax=axes[0], marker='o')
        axes[0].set_title('Weekly Case Volume Trend')
        axes[0].set_xlabel('Week Number')
        axes[0].set_ylabel('Number of Cases')
        axes[0].grid(True, alpha=0.3)
        
        # ë³¼ë¥¨ ë¶„í¬
        weekly_volume.hist(bins=20, ax=axes[1])
        axes[1].set_title('Weekly Volume Distribution')
        axes[1].set_xlabel('Cases per Week')
        axes[1].set_ylabel('Frequency')
        axes[1].axvline(weekly_volume.mean(), color='red', linestyle='--', label='Mean')
        axes[1].legend()
        
        plt.tight_layout()
        plt.show()
        
        # ì•ˆì •ì„± ê²½ê³ 
        cv = weekly_volume.std() / weekly_volume.mean()
        if cv > 0.3:
            print(f"âš ï¸ ê²½ê³ : ì£¼ë³„ ë°ì´í„° ë³¼ë¥¨ ë³€ë™ì´ í½ë‹ˆë‹¤ (CV: {cv:.3f})")
        else:
            print(f"âœ… ì£¼ë³„ ë°ì´í„° ë³¼ë¥¨ì´ ì•ˆì •ì ì…ë‹ˆë‹¤ (CV: {cv:.3f})")

analyze_temporal_volume_stability()

# 9. ì¢…í•© ì•ˆì •ì„± ë¦¬í¬íŠ¸
def generate_stability_report(train_base, test_base, stability_results):
    print("\n" + "="*60)
    print("HOME CREDIT ëª¨ë¸ ì•ˆì •ì„± ì¢…í•© ë¦¬í¬íŠ¸")
    print("="*60)
    
    # 1. ë°ì´í„° ê°œìš”
    print(f"\n1. ë°ì´í„° ê°œìš”")
    if train_base is not None and test_base is not None:
        print(f"   â€¢ Train ì¼€ì´ìŠ¤: {len(train_base):,}ê±´")
        print(f"   â€¢ Test ì¼€ì´ìŠ¤: {len(test_base):,}ê±´")
        
        if 'target' in train_base.columns:
            print(f"   â€¢ ì „ì²´ ë¶€ì‹¤ë¥ : {train_base['target'].mean():.4f} ({train_base['target'].mean()*100:.2f}%)")
        
        if 'WEEK_NUM' in train_base.columns:
            train_weeks = train_base['WEEK_NUM'].nunique()
            test_weeks = test_base['WEEK_NUM'].nunique()
            print(f"   â€¢ Train ê¸°ê°„: {train_weeks}ì£¼")
            print(f"   â€¢ Test ê¸°ê°„: {test_weeks}ì£¼")
    
    # 2. ì•ˆì •ì„± í‰ê°€
    print(f"\nâš–ï¸ 2. ì•ˆì •ì„± í‰ê°€")
    if len(stability_results) > 0:
        stable_count = len(stability_results[stability_results['stability'] == 'Stable'])
        moderate_count = len(stability_results[stability_results['stability'] == 'Moderately Unstable'])
        unstable_count = len(stability_results[stability_results['stability'] == 'Highly Unstable'])
        total_features = len(stability_results)
        
        print(f"   â€¢ ì•ˆì •ì  í”¼ì²˜: {stable_count}/{total_features} ({stable_count/total_features*100:.1f}%)")
        print(f"   â€¢ ì•½ê°„ ë¶ˆì•ˆì •: {moderate_count}/{total_features} ({moderate_count/total_features*100:.1f}%)")
        print(f"   â€¢ ë§¤ìš° ë¶ˆì•ˆì •: {unstable_count}/{total_features} ({unstable_count/total_features*100:.1f}%)")
        
        # ê°€ì¥ ë¶ˆì•ˆì •í•œ í”¼ì²˜ë“¤
        if unstable_count > 0:
            worst_features = stability_results.nlargest(3, 'psi')
            print(f"\n   âš ï¸ ê°€ì¥ ë¶ˆì•ˆì •í•œ í”¼ì²˜ë“¤:")
            for _, row in worst_features.iterrows():
                print(f"      - {row['feature']} (PSI: {row['psi']:.3f}, Table: {row['table_group']})")
    else:
        print(f"   â€¢ ì•ˆì •ì„± ë¶„ì„ ê²°ê³¼ ì—†ìŒ (ë°ì´í„° ë¡œë“œ ë¬¸ì œ)")
    
    # 3. ìœ„í—˜ ìš”ì†Œ
    print(f"\nğŸš¨ 3. ì£¼ìš” ìœ„í—˜ ìš”ì†Œ")
    risks = []
    
    if len(stability_results) > 0:
        high_risk_features = len(stability_results[stability_results['psi'] > 0.25])
        if high_risk_features > 0:
            risks.append(f"ë†’ì€ PSI ê°’ì„ ê°€ì§„ {high_risk_features}ê°œ í”¼ì²˜")
    
    if train_base is not None and 'WEEK_NUM' in train_base.columns:
        weekly_volume = train_base.groupby('WEEK_NUM').size()
        cv = weekly_volume.std() / weekly_volume.mean()
        if cv > 0.3:
            risks.append(f"ë†’ì€ ë°ì´í„° ë³¼ë¥¨ ë³€ë™ì„± (CV: {cv:.3f})")
    
    if len(risks) > 0:
        for risk in risks:
            print(f"   â€¢ {risk}")
    else:
        print(f"   â€¢ ì£¼ìš” ìœ„í—˜ ìš”ì†Œ ì—†ìŒ")
    
    # 4. ê¶Œì¥ì‚¬í•­
    print(f"\n 4. ê¶Œì¥ì‚¬í•­")
    print(f"   â€¢ ì •ê¸°ì ì¸ ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ (ìµœì†Œ ì›” 1íšŒ)")
    print(f"   â€¢ PSI > 0.25ì¸ í”¼ì²˜ë“¤ì— ëŒ€í•œ íŠ¹ë³„ ê´€ë¦¬")
    print(f"   â€¢ ìƒˆë¡œìš´ ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€ ì‹œ ì•ˆì •ì„± ì‚¬ì „ ê²€ì¦")
    print(f"   â€¢ A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ëª¨ë¸ ì—…ë°ì´íŠ¸ ê²€ì¦")
    print(f"   â€¢ ë°±í…ŒìŠ¤íŒ…ì„ í†µí•œ ì‹œê°„ë³„ ì„±ëŠ¥ ì•ˆì •ì„± í™•ì¸")
    
    # 5. ë‹¤ìŒ ë‹¨ê³„
    print(f"\n 5. ë‹¤ìŒ ë‹¨ê³„")
    print(f"   â€¢ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ìµœì í™”")
    print(f"   â€¢ ì•™ìƒë¸” ëª¨ë¸ì„ í†µí•œ ì•ˆì •ì„± í–¥ìƒ")
    print(f"   â€¢ ë„ë©”ì¸ ì „ë¬¸ê°€ì™€ì˜ í”¼ì²˜ ê²€í† ")
    print(f"   â€¢ í”„ë¡œë•ì…˜ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•")

generate_stability_report(train_base, test_base, stability_results)

print(f"\n" + "="*60)
print(" EDA ì™„ë£Œ! ëª¨ë¸ ì•ˆì •ì„± ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
print("="*60)